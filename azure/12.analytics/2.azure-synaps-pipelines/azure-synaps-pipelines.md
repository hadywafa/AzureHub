# ü™à **Azure Synapse Pipelines** ‚Äî The ETL Engine of Synapse

## üåç What Are Synapse Pipelines?

> **Synapse Pipelines** are the **data orchestration and integration** component inside Azure Synapse Analytics.
> They‚Äôre built on top of **Azure Data Factory (ADF)** ‚Äî but live directly _inside_ your Synapse workspace.

You can think of them as:

> ‚Äú**ADF inside Synapse**‚Äù ‚Äî same engine, same activities, but natively integrated with Synapse SQL, Spark, and Data Lake.

---

## üß† Why Do We Need Pipelines?

Imagine you have:

- Raw data coming from **on-prem SQL Server**
- Clean data stored in **Azure Data Lake**
- Aggregated data hosted in **Synapse SQL Pool**
- Dashboards running in **Power BI**

üëâ Synapse Pipelines **automate** all the movement and transformation between these systems.

---

## ‚öôÔ∏è ETL vs. ELT in Synapse Pipelines

| Pattern                            | Description                                            | Typical Use               |
| ---------------------------------- | ------------------------------------------------------ | ------------------------- |
| **ETL (Extract, Transform, Load)** | Transform data _before_ loading into Synapse           | Using Spark or Data Flow  |
| **ELT (Extract, Load, Transform)** | Load data _first_, then transform _inside Synapse SQL_ | For large structured data |

üß© Synapse Pipelines can do both, depending on your scenario.

---

## üß© Key Components of a Synapse Pipeline

| Component                    | Description                                                     | Analogy              |
| ---------------------------- | --------------------------------------------------------------- | -------------------- |
| **Pipeline**                 | A logical group of activities (the full workflow)               | Recipe               |
| **Activity**                 | A single operation (copy, run notebook, execute SQL, etc.)      | Step in recipe       |
| **Linked Service**           | Connection info for data sources (like credentials + endpoints) | Kitchen appliance    |
| **Dataset**                  | Structure of input/output data (schema, file path)              | Ingredients          |
| **Trigger**                  | Schedule or event that starts pipeline                          | Timer or alarm clock |
| **Integration Runtime (IR)** | Compute engine that executes data movement                      | Chef                 |

---

## üß± Visual Architecture

```mermaid
---
config:
  theme: dark
---
flowchart LR
    A["Source Systems<br>SQL Server, Blob, SAP, APIs"] -->|Extract| B[Synapse Pipeline]
    B -->|Transform| C["Spark Notebook / Data Flow"]
    C -->|Load| D["Data Lake / Synapse SQL / Power BI"]
    B -->|Triggers| E[(Schedule / Event)]
```

---

## üß© Common Activities (Building Blocks)

| Category                | Example Activity                   | Purpose                   |
| ----------------------- | ---------------------------------- | ------------------------- |
| **Data Movement**       | Copy Data                          | Move data between sources |
| **Data Transformation** | Data Flow / Spark Notebook         | Clean or aggregate data   |
| **Control Flow**        | ForEach, If, Until                 | Logic and looping         |
| **Execution**           | Stored Procedure, Synapse Notebook | Run SQL or code           |
| **Monitoring**          | Set Variable, Web Activity         | Manage state and logs     |

---

## üß™ Example 1: Copy Data from Blob to Synapse SQL

### Step 1Ô∏è‚É£ ‚Äî Create Linked Services

- One for **Azure Blob Storage**
- One for **Synapse SQL Pool**

### Step 2Ô∏è‚É£ ‚Äî Create Datasets

- Input dataset ‚Üí Blob CSV
- Output dataset ‚Üí SQL table

### Step 3Ô∏è‚É£ ‚Äî Add **Copy Data Activity**

- Source ‚Üí Blob
- Sink ‚Üí SQL
- Map columns automatically or manually

### Step 4Ô∏è‚É£ ‚Äî Publish and Run

- Click **‚ÄúPublish All‚Äù**
- Run the pipeline manually or attach a trigger

‚úÖ Data moves from Blob ‚Üí Synapse automatically.

---

### Example SQL Table

```sql
CREATE TABLE Sales (
  Id INT,
  ProductName NVARCHAR(100),
  Amount DECIMAL(10,2)
);
```

---

## ‚öôÔ∏è Example 2: Orchestrate a Full ETL Workflow

**Goal:**

1. Copy raw files from Blob ‚Üí Data Lake
2. Trigger a Databricks or Synapse Spark notebook for cleaning
3. Load results into Synapse SQL
4. Refresh Power BI dataset

```mermaid
---
config:
  theme: dark
---
flowchart LR
    A[Copy Data from Blob] --> B["Run Synapse Notebook<br>(Spark Transform)"]
    B --> C[Load Clean Data into SQL Pool]
    C --> D[Refresh Power BI Dataset]
```

‚úÖ You can control all 4 steps in one pipeline ‚Äî no manual work.

---

## ‚öôÔ∏è Example 3: Trigger a Databricks or Synapse Notebook

```json
{
  "name": "RunSparkNotebook",
  "type": "SynapseNotebook",
  "dependsOn": [],
  "policy": { "timeout": "01:00:00" },
  "typeProperties": {
    "notebook": {
      "referenceName": "clean_sales_notebook",
      "type": "NotebookReference"
    }
  }
}
```

‚úÖ This runs your Databricks or Synapse Spark notebook automatically after data is copied.

---

## üïí Triggers ‚Äî How Pipelines Start

| Type                 | Description                | Example                           |
| -------------------- | -------------------------- | --------------------------------- |
| **Manual Trigger**   | Run on demand              | ‚ÄúRun Now‚Äù button                  |
| **Schedule Trigger** | Run by time                | Every day at 2 AM                 |
| **Tumbling Window**  | Repeats with intervals     | Every 15 min for real-time ingest |
| **Event Trigger**    | Starts when a file arrives | When new CSV is uploaded to Blob  |

---

## üìä Monitoring Pipelines

- Navigate to **Synapse Studio ‚Üí Monitor ‚Üí Pipeline Runs**
- You‚Äôll see:

  - ‚úÖ Success/failure status
  - ‚è± Duration
  - ‚ö†Ô∏è Error messages (if any)

- You can click any activity to drill down into logs.

---

## üß± Integration Runtime (IR) ‚Äî The Compute Engine

| Type                | Use Case                   |                                |
| ------------------- | -------------------------- | ------------------------------ |
| **Auto-resolve IR** | Default; Azure-managed     | Works for cloud data transfers |
| **Self-hosted IR**  | For on-prem data sources   | Installed on your VM           |
| **Azure-SSIS IR**   | Run existing SSIS packages | For legacy workloads           |

‚úÖ You rarely need to worry ‚Äî Synapse usually uses _Auto-resolve IR_ automatically.

---

## üßÆ Example End-to-End Pipeline (Simplified JSON)

```json
{
  "name": "DailySalesPipeline",
  "activities": [
    { "name": "CopySalesData", "type": "Copy", "inputs": ["RawBlob"], "outputs": ["StagingLake"] },
    { "name": "CleanData", "type": "SynapseNotebook", "dependsOn": ["CopySalesData"] },
    { "name": "LoadToWarehouse", "type": "SqlServerStoredProcedure", "dependsOn": ["CleanData"] }
  ]
}
```

‚úÖ This pipeline copies, cleans, and loads daily sales data ‚Äî all scheduled automatically.

---

## üß© Synapse Pipelines vs Azure Data Factory (ADF)

| Feature         | **Synapse Pipelines**               | **Azure Data Factory**            |
| --------------- | ----------------------------------- | --------------------------------- |
| **Interface**   | Inside Synapse Studio               | Standalone portal                 |
| **Engine**      | Same as ADF                         | Same as Synapse                   |
| **Best For**    | Analytics pipelines tied to Synapse | Enterprise-wide ETL orchestration |
| **Integration** | Direct with Synapse SQL & Spark     | Broad across Azure services       |
| **Cost**        | Pay per run / per activity          | Same pricing model                |

üß† **In short:**
If your goal is analytics inside Synapse, use **Synapse Pipelines**.
If your goal is enterprise ETL orchestration across 50 systems, use **ADF**.

---

## üîê Security & Governance

| Feature                     | Description                               |
| --------------------------- | ----------------------------------------- |
| **Azure AD Authentication** | Secure connections and RBAC               |
| **Managed Identity**        | Avoid storing credentials                 |
| **Private Endpoints**       | Secure communication                      |
| **Activity Logging**        | Monitor via Azure Monitor & Log Analytics |

---

## üí∏ Cost Model

| Item                    | Pricing Basis               | Example                  |
| ----------------------- | --------------------------- | ------------------------ |
| **Pipeline Runs**       | Per activity execution      | Copy, notebook, SQL job  |
| **Data Movement**       | Volume-based                | e.g., \$0.25 per 50 runs |
| **Integration Runtime** | Pay per hour if self-hosted | Based on compute size    |

‚úÖ For most Synapse users, cost is minimal unless pipelines run 24/7.

---

## üß† Typical Real-World Use Case

| Step | Task                          | Service              |
| ---- | ----------------------------- | -------------------- |
| 1Ô∏è‚É£   | Copy raw CSV from on-prem SQL | Pipeline (Copy Data) |
| 2Ô∏è‚É£   | Clean data using PySpark      | Synapse Notebook     |
| 3Ô∏è‚É£   | Store clean data in Delta     | Data Lake            |
| 4Ô∏è‚É£   | Load summary to SQL Pool      | Stored Procedure     |
| 5Ô∏è‚É£   | Refresh Power BI dataset      | REST / API Activity  |

All steps automated. No manual work. No cron jobs. Just **pipelines**.

---

## ‚úÖ Summary

| Concept               | Description                                                |
| --------------------- | ---------------------------------------------------------- |
| **Synapse Pipelines** | Visual, code-free ETL engine inside Synapse                |
| **Built on**          | Azure Data Factory                                         |
| **Core parts**        | Pipelines, Activities, Linked Services, Datasets, Triggers |
| **Purpose**           | Move and transform data between Azure sources              |
| **Integrates with**   | SQL Pools, Spark, Data Lake, Power BI                      |
| **Pricing**           | Pay per run / per activity                                 |
| **Security**          | Azure AD + Managed Identity                                |

---

üí¨ **In one line:**

> **Synapse Pipelines** are your automated data movers ‚Äî
> They extract, transform, and load your data across Azure in a scheduled, monitored, and fully managed way.
