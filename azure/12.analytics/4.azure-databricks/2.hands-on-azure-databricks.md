# ğŸ§  Hands-on Guide: Connect Azure Databricks to Azure Data Lake Storage (ADLS Gen2)

## ğŸ§± Step 1. Create the Prerequisites

### âœ… Youâ€™ll Need:

| Resource                              | Purpose                          |
| ------------------------------------- | -------------------------------- |
| **Azure Storage Account (ADLS Gen2)** | Where your data files live       |
| **Azure Databricks Workspace**        | Your data processing environment |
| **Resource Group**                    | Logical container for both       |

---

### ğŸªœ 1.1 Create a Storage Account (ADLS Gen2)

1. Go to **Azure Portal â†’ Create a resource â†’ Storage Account**

2. Fill these important fields:

   - **Resource Group:** `rg-databricks-demo`
   - **Storage account name:** `stdataengdemo`
   - **Region:** Same as where youâ€™ll create Databricks (e.g., East US)
   - **Performance:** Standard
   - **Redundancy:** LRS
   - **Enable hierarchical namespace:** âœ… (this makes it _Data Lake Storage Gen2_)

3. Click **Review + Create â†’ Create**

4. After deployment â†’ open the storage account and note down:

   - `Storage account name`
   - `Container name` (create one, e.g., `rawdata`)

---

### ğŸªœ 1.2 Create an Azure Databricks Workspace

1. Go to **Azure Portal â†’ Create a resource â†’ Databricks**
2. Fill:

   - **Workspace name:** `adb-demo`
   - **Region:** Same as your storage account
   - **Pricing tier:** _Premium_ (so you can use access controls)

3. Click **Review + Create â†’ Create**
4. When ready â†’ click **Launch Workspace**

Youâ€™re now inside **Databricks UI** ğŸ‰

---

## âš™ï¸ Step 2. Create a Databricks Cluster

### Why?

A **cluster** is a group of virtual machines that runs your Spark jobs.

### Steps:

1. In Databricks, go to **Compute â†’ Create Cluster**
2. Set:

   - **Cluster name:** `demo-cluster`
   - **Cluster mode:** Standard
   - **Runtime version:** Latest _Databricks Runtime (with Apache Spark and Scala)_
   - **Auto-termination:** 15 min (to save cost)

3. Click **Create Cluster**
4. Wait 5â€“10 minutes until itâ€™s _running_

---

## ğŸ“¦ Step 3. Mount the Data Lake to Databricks

Now weâ€™ll make the Data Lake accessible like a **local folder** inside Databricks.

There are **two methods**:

1. ğŸ” Using **Account Key** (simpler for learning)
2. ğŸ›¡ï¸ Using **Service Principal (AAD)** (for production)

Weâ€™ll use method 1 now.

---

### ğŸªœ 3.1 Get the Storage Access Key

- Go to your **Storage Account â†’ Access Keys**
- Copy one of the **keys**

---

### ğŸªœ 3.2 Create a Notebook in Databricks

1. Click **Workspace â†’ Create â†’ Notebook**
2. Name it `mount_adls_demo`
3. Language: `Python`
4. Attach your running cluster.

---

### ğŸªœ 3.3 Run the Following Code

```python
# Replace these values
storage_account_name = "stdataengdemo"
container_name = "rawdata"
storage_account_key = "<paste-your-access-key>"

# Mount the ADLS Gen2 container
dbutils.fs.mount(
  source = f"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/",
  mount_point = f"/mnt/{container_name}",
  extra_configs = {f"fs.azure.account.key.{storage_account_name}.blob.core.windows.net": storage_account_key}
)

print("âœ… ADLS mounted successfully!")
```

ğŸ§© Result:
A virtual folder `/mnt/rawdata` appears inside Databricks â€” representing your ADLS container.

---

## ğŸ“Š Step 4. Test Reading and Writing Data

Letâ€™s make sure your Databricks can actually access the lake.

### ğŸ§¾ Upload a file to ADLS

In your storage account:

- Go to **Containers â†’ rawdata â†’ Upload**
- Upload a sample CSV (e.g., `sales.csv`)

---

### ğŸ’» In Databricks Notebook:

```python
# Read the CSV file
df = spark.read.csv("/mnt/rawdata/sales.csv", header=True, inferSchema=True)
display(df)
```

âœ… Youâ€™ll see your CSV as a dataframe table.

Now, write it back after cleaning ğŸ‘‡

```python
# Write cleaned data to a new location
df.write.mode("overwrite").parquet("/mnt/rawdata/cleaned_sales")
print("âœ… Data written back to ADLS as Parquet")
```

âœ… Check in Azure Portal â†’ container â†’ youâ€™ll find a folder `cleaned_sales/`.

---

## ğŸ§® Step 5. (Optional) Query Data Using SQL

You can run SQL queries directly in Databricks!

1. Create a new cell â†’ change language to **SQL**
2. Run this:

```sql
CREATE OR REPLACE TEMP VIEW sales_view
USING parquet
OPTIONS (path "/mnt/rawdata/cleaned_sales")

SELECT country, SUM(amount) AS total_sales
FROM sales_view
GROUP BY country
ORDER BY total_sales DESC;
```

âœ… Youâ€™ll instantly get summarized results â€” just like SQL Server or Power BI.

---

## ğŸ§  What You Just Learned

| Concept            | What Happened                        |
| ------------------ | ------------------------------------ |
| **Mounting**       | Connected ADLS Gen2 to Databricks    |
| **Dataframe**      | Loaded data from CSV to Spark memory |
| **Transformation** | Processed and saved data as Parquet  |
| **SQL View**       | Queried data using SQL syntax        |
| **Cluster**        | Ran all the Spark jobs for you       |

---

## ğŸ Summary Diagram

```mermaid
---
config:
  theme: dark
---
flowchart LR
    A[Azure Data Lake Storage Gen2] -->|Mount via Access Key| B[Azure Databricks Cluster]
    B -->|Read CSV| C[DataFrame]
    C -->|Write Cleaned Data| A
    C -->|SQL Queries| D[Databricks SQL / Power BI]
```

---

## ğŸ’¬ Whatâ€™s Next

Now that your Databricks and Data Lake are connected, you can:

1. **Automate the job** using **Databricks Jobs**
2. **Use Delta Lake** format to track data versions (next-level reliability)
3. **Connect Power BI** directly to visualize your cleaned datasets

---

Would you like me to teach you **Delta Lake (in Databricks)** next â€”
so you can understand how data versioning, ACID transactions, and "time travel" work inside a Data Lake?
Thatâ€™s the next key concept in becoming a real Data Engineer ğŸš€
