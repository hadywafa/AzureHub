# ğŸ“ **Azure Event Hubs Capture**

## ğŸ“Œ **What It Is**

> _ğŸ“– Official Definition:  
> Event Hubs Capture enables you to automatically capture the streaming data in Event Hubs and save it to your choice of either a Blob storage account, or an Azure Data Lake Storage account._

Event Hubs **Capture** is a **built-in auto-archival feature** that:

- Collects all streaming data (from each partition).
- Batches them by **time window** or **file size window**.
- Stores them in **Azure Blob Storage** or **Data Lake Storage Gen2**.
- Writes files in **Apache Avro format** (compact, binary, schema-based).
- (Bonus ğŸ‰) If you use **no-code editor in Azure Portal**, you can save in **Parquet format** for direct analytics with Synapse. [Learn More](https://learn.microsoft.com/en-us/azure/stream-analytics/capture-event-hub-data-parquet?toc=%2Fazure%2Fevent-hubs%2Ftoc.json)

> ğŸ‘‰ This turns your raw event stream into **structured, queryable files** â€” ready for **batch analytics, ML pipelines, or archival**.

---

<div align="center" style="background-color: #ffffffff ;border-radius: 10px;border: 2px solid white">
  <img src="image/2.event-hub-capture/1758893638571.png" alt="Capture Configuration" style="border-radius: 10px; border: 2px solid white; width: 80%;margin:0 30px">
</div>

---

## âš™ï¸ **How Capture Works** (Internals)

### 1. **Windowing**

- Data is written **per partition**.
- Two triggers decide when to write a file:

  - **Time Window** â†’ e.g., every 5 minutes.
  - **Size Window** â†’ e.g., every 10 MB.

- At the end of the window â†’ Event Hubs flushes events into a blob.

ğŸ•’ Example:

- Partition 0 â†’ flushes every 5 min (even if not full).
- Partition 1 â†’ flushes when 10 MB reached (even before 5 min).

> ğŸ“‚ File name includes: namespace / hub name / partition / timestamp.

---

### 2. **Scaling with Throughput Units**

- Capture works **independent of TU quotas**.
- Even if youâ€™re hammering Event Hubs with millions of events/sec â†’ Capture wonâ€™t â€œstealâ€ your processing throughput.
- Files still land in storage consistently.

---

### 3. **Automatic Start**

- No code, no extra pipeline.
- As soon as the **first event** hits Event Hubs â†’ Capture starts writing files.

---

### 4. **Data Format**

- Default: **Apache Avro** â†’ compact binary, schema is embedded.
- Optional: **Parquet** (if using no-code path in portal) â†’ perfect for querying via Synapse/Databricks.

ğŸ“Š Avro Schema (simplified):

```json
{
  "records": [
    {
      "time": "2024-09-25T16:06:06Z",
      "resourceId": "/subscriptions/xxx/resourceGroups/RG/providers/Microsoft.Web/sites/App",
      "category": "AppServiceAppLogs",
      "operationName": "Microsoft.Web/sites/log",
      "level": "Information",
      "message": "Application started..."
    }
  ]
}
```

---

## ğŸ› ï¸ **Hands-On Setup** (Portal)

### Step 1: Upgrade Tier

- Ensure Event Hub is **Standard or higher** (Capture not in Basic).

### Step 2: Create an Event Hub

- Example: Name = **Hub01**
- Partitions = 1 (for demo).
- Retention = 1 hr (cleanup policy = delete).

### Step 3: Enable Capture

- Check â€œEnable Capture.â€
- Configure:

  - **Window size** â†’ 5 minutes.
  - **Size window** â†’ 10 MB.

- Select **Storage account / Data Lake container**.
- Authentication for Capture: SAS or Managed Identity.

<div align="left">
  <img src="image/2.event-hub-capture/1758907372680.png" alt="Capture Configuration" style="border-radius: 10px; border: 2px solid white; width: 80%;margin:0 30px">
</div>

---

## ğŸ”¬ **Demo** (Testing Capture)

### 1. Generate Events

- Option A â†’ .NET SDK, Python SDK.
- Option B â†’ Diagnostic Settings from another Azure service (e.g., App Service logs â†’ Event Hub).

Example: configure App Service diagnostic settings â†’ send logs to **Hub01**.

### 2. Verify Data

- Go to your Storage account â†’ container.
- Youâ€™ll see folders like:

```ini
/eventhub-namespace/Hub01/0/2024/09/25/16/06/06.avro
```

### 3. Open Data

- Download `.avro` file.
- Tools: Azure Data Explorer, Avro tools, or load into Databricks/Synapse.

---

## ğŸ”„ **Capture Window Tuning**

- **Small windows** â†’ More, smaller files (good for near real-time).
- **Large windows** â†’ Fewer, larger files (good for batch jobs).

Example:

- Demo: 1 min / 10 MB â†’ fast feedback.
- Production: 5 min / 100 MB â†’ efficient storage.

---

## ğŸ“Š **Comparison with AWS**

| Feature             | Event Hubs Capture  | AWS Equivalent            |
| ------------------- | ------------------- | ------------------------- |
| Storage Destination | Blob / Data Lake    | S3 (via Kinesis Firehose) |
| File Format         | Avro / Parquet      | Parquet / ORC             |
| Window Trigger      | Time / Size         | Buffer Size / Interval    |
| Setup               | One-click in Portal | Needs Firehose config     |

---

## ğŸ† **Key Benefits**

- âœ… **No code ingestion pipeline** (native to Event Hubs).
- âœ… **Reliable + Scalable** (doesnâ€™t eat TUs).
- âœ… **Batch-friendly** â†’ great for analytics.
- âœ… **Format-ready** â†’ Avro (compact) or Parquet (analytics).

---

ğŸ‘‰ In short: **Event Hubs Capture = Your Always-On â€œFlight Recorderâ€**. It guarantees that no event stream is lost and makes analysis easy with downstream Azure services.

---

Would you like me to also give you a **CLI hands-on guide** (az cli commands) for enabling and verifying Event Hubs Capture â€” so you see the automation side as well, not just Portal setup?
