# üß≠ What is a checkpoint (recap)?

A **checkpoint** is the saved position (offset + sequence number) telling ‚ÄúI finished up to here in _this partition_ for _this consumer group_.‚Äù

- Stored **outside** Event Hubs (most commonly in **Azure Blob Storage**).
- Used so the **next** consumer (after failover/restart/scale) restarts at the right place.

---

## üß© Two kinds of state in the Blob store

Event Hubs SDKs (e.g., `EventProcessorClient`) keep **two** logical record types in the checkpoint store:

1. **Partition Ownership** (who is the active reader for a partition)

   - One blob per partition per consumer group.
   - Uses **optimistic concurrency (ETags)** to avoid two owners at once.
   - Refreshed periodically (renew/steal on rebalance).

2. **Checkpoint** (where we got up to)

   - One blob per partition per consumer group.
   - Stores **offset** and **sequence number**.
   - Updated by the **current owner** _after_ it finishes some work.

> Think: **Ownership** = lock/lease; **Checkpoint** = bookmark.

---

## üõ°Ô∏è How the SDK prevents conflicts

### 1. Single active reader per partition (per consumer group)

- The processor **claims ownership** of a partition by writing an ownership blob with an **ETag**.
- Another instance can‚Äôt ‚Äúalso‚Äù own it unless it **wins a race** with a newer ETag (during rebalance/steal).
- Result: under normal operation **only one instance writes checkpoints** for that partition ‚Üí **no conflicts**.

### 2. Optimistic concurrency with ETags

- Ownership updates use ETags ‚Üí **compare-and-swap** behavior.
- If two instances try to claim/update at the same time, **only one write succeeds**; the other gets a precondition failure and backs off.
- This is how the SDK coordinates partition **rebalancing** safely.

### 3. Checkpoint writes are simple and safe

- Checkpoints don‚Äôt need a lease; they‚Äôre written by the **current owner**.
- Because there‚Äôs (normally) only one owner, you don‚Äôt get concurrent checkpoint writers.
- If you **intentionally** run multiple readers for the same partition in one consumer group (supported but not recommended), **you** must handle dedupe/races (see below).

---

## üîÅ What happens during failover / scale events?

### VM dies (owner disappears)

1. Its **ownership** blobs expire (short window; default behavior managed by the SDK).
2. Another instance claims those partitions (wins the ETag race).
3. The new owner reads the **last checkpoint** and resumes **from there**.

> You may reprocess a few events if the dead consumer hadn‚Äôt checkpointed its latest progress.
> This is normal **at-least-once** delivery; design your processing to be **idempotent**.

### Scale out (add more VMs)

- The load balancer **reassigns** partitions so each instance has a fair share.
- Reassigned owners read the **same partition** but **start at the last checkpoint**.
- No duplicates beyond the usual ‚Äúsince last checkpoint‚Äù window.

---

## ‚ùì‚ÄúCan I have multiple consumers on the same partition in one group?‚Äù

You _can_ (up to 5), but then:

- **All** readers see **all** events for that partition.
- If more than one tries to checkpoint, you‚Äôll have **races** (last writer wins).
- You must implement **application-level deduplication** and a strategy for **which reader checkpoints** (e.g., designate one ‚Äúcheckpointer,‚Äù or checkpoint in a transactional outbox table your app controls).

**Recommendation:** Keep **one active reader per partition per consumer group** and let the SDK manage it.

---

## üßÆ Offsets vs sequence numbers (why both?)

- **Offset** = byte position in the log (precise resume point).
- **Sequence number** = monotonic event index (great for logs/metrics/dedupe keys).
- Checkpoints typically store **both**. The SDK resumes from the **offset**.

---

## üß∞ Best-practice playbook (battle-tested)

1. **One owner per partition per group**
   Let the SDK manage ownership; don‚Äôt start multiple readers for the same partition in one group.

2. **Checkpoint after processing** (not before)
   Ensures a crash never advances the bookmark past unprocessed work.

3. **Checkpoint periodically**
   Time-based (e.g., every 5‚Äì30s) or count-based (e.g., every 100‚Äì500 events).
   Per-event checkpoints are expensive; too rare ‚Üí more reprocessing on failover.

4. **Idempotent processing**
   Use a **natural dedupe key** (e.g., `OrderId` + `Status` + `SequenceNumber`) and make downstream writes **upserts**.
   If you must fire external side effects (emails, webhooks), persist a **sent ledger** keyed by event id.

5. **One Blob container per consumer group**
   (And don‚Äôt mix it with unrelated data.) Put it in the **same region** as the consumers.

6. **Tune load balancing** (advanced)
   SDK options let you adjust **update intervals** and **ownership expiration** if you need faster/fewer rebalances.

---

## üßë‚Äçüíª Minimal examples

### C# ‚Äî EventProcessorClient with Blob checkpoints

```csharp
var storage = new BlobContainerClient(connStr, "cg-a-checkpoints");
var checkpointStore = new BlobCheckpointStore(storage);

var processor = new EventProcessorClient(
    checkpointStore,
    consumerGroup: EventHubConsumerClient.DefaultConsumerGroupName,
    connectionString: ehConnStr,
    eventHubName: "orders");

processor.ProcessEventAsync += async args =>
{
    // 1) Process safely & idempotently
    var data = args.Data;
    // ... your processing ...

    // 2) Periodic checkpoint (e.g., every 200 events)
    if (args.Partition.PartitionId.GetHashCode() % 200 == 0)
    {
        await args.UpdateCheckpointAsync(args.CancellationToken);
    }
};

processor.ProcessErrorAsync += args =>
{
    Console.WriteLine($"Err {args.Exception.Message} on {args.PartitionId}");
    return Task.CompletedTask;
};

await processor.StartProcessingAsync();
// later: await processor.StopProcessingAsync();
```

**Notes:**

- The SDK claims **ownership**; only that instance writes checkpoints for its partitions ‚Üí no conflicts.
- `UpdateCheckpointAsync` writes the **offset + sequence number** from the **last processed** event.

### Python ‚Äî EventHubConsumerClient (manual checkpoint idea)

```python
from azure.eventhub.aio import EventHubConsumerClient

async def on_event(partition_context, event):
    # Process idempotently using event.system_properties["x-opt-sequence-number"] or a business key
    # ...

    # Periodic checkpoint
    if event.sequence_number % 200 == 0:
        await partition_context.update_checkpoint(event)

client = EventHubConsumerClient.from_connection_string(
    conn_str=EH_CONN_STR,
    consumer_group="$Default",
    eventhub_name="orders",
    checkpoint_store=blob_checkpoint_store)

async with client:
    await client.receive(on_event=on_event)
```

---

## üß® What about ‚Äúexactly-once‚Äù?

- Event Hubs (like Kafka/Kinesis) offers **at-least-once** semantics.
- **Exactly-once** is achieved **in your app** via **idempotency** (dedupe keys, upserts, transactional outbox patterns).
- Expect that **after failover**, the new owner may **replay a few events** (since last checkpoint). Your design must tolerate that.

---

## üß† TL;DR

- **Conflicts are avoided** by **one owner per partition** + **ETag-based** ownership.
- **Checkpoints** are written by the **current owner**; with recommended topology there‚Äôs no writer race.
- On **failure or scale**, the SDK **reassigns** partitions and resumes from the **last checkpoint** ‚Üí at-least-once delivery; design for **idempotency**.
- If you **intentionally** run multiple readers per partition, **you** must coordinate checkpointing and dedupe.
