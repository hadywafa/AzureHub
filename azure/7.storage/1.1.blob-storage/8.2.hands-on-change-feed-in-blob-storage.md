# 🧪 Demo: Azure Blob Storage Change Feed

We’ll simulate this scenario:
👉 A storage account has invoices uploaded/deleted.
👉 We enable Change Feed.
👉 A .NET/Python app reads all events from `$blobchangefeed` to detect changes.

---

## 🟢 Step 1: Enable Change Feed

1. Go to **Azure Portal** → Storage Account.
2. Navigate to **Data Management → Change feed**.
3. Toggle **Enable** ✅.

![Enable Change Feed](https://learn.microsoft.com/en-us/azure/storage/blobs/media/storage-blob-change-feed/change-feed-portal.png)

- By default, all containers in the storage account are monitored.
- A **system container** `$blobchangefeed` will be created (hidden in portal, but accessible via SDK/CLI).

---

## 🟢 Step 2: Upload and Delete Blobs

Let’s simulate blob changes:

```bash
# Upload a new blob
az storage blob upload \
  --account-name mystorageacct \
  --container-name invoices \
  --name invoice-101.pdf \
  --file ./invoice-101.pdf

# Delete a blob
az storage blob delete \
  --account-name mystorageacct \
  --container-name invoices \
  --name invoice-101.pdf
```

✅ These operations generate **BlobCreated** and **BlobDeleted** events in the Change Feed.

---

## 🟢 Step 3: Inspect the Change Feed Container

The system container looks like this:

```ini
$blobchangefeed/
  idx/  (index files)
  log/  (segments with event data)
```

Each log file contains **JSON array of events**.

Example raw event:

```json
{
  "schemaVersion": 1,
  "topic": "/subscriptions/<sub>/resourceGroups/<rg>/providers/Microsoft.Storage/storageAccounts/mystorage",
  "subject": "/blobServices/default/containers/invoices/blobs/invoice-101.pdf",
  "eventType": "BlobDeleted",
  "eventTime": "2025-10-03T12:34:56Z",
  "id": "abc123",
  "data": {
    "api": "DeleteBlob",
    "contentLength": 2048,
    "blobType": "BlockBlob",
    "sequencer": "0000000000000001000000000000000000000000000"
  }
}
```

---

## 🟢 Step 4: Consume Events via SDK

### Option 1: .NET SDK

```csharp
using Azure.Storage.Blobs.ChangeFeed;
using Azure.Storage.Blobs;
using System;
using System.Threading.Tasks;

class Program
{
    static async Task Main()
    {
        string connectionString = "<your-connection-string>";
        BlobServiceClient serviceClient = new BlobServiceClient(connectionString);

        // Get Change Feed Client
        ChangeFeedClient changeFeedClient = serviceClient.GetChangeFeedClient();

        // Read changes
        await foreach (BlobChangeFeedEvent changeEvent in changeFeedClient.GetChangesAsync())
        {
            Console.WriteLine($"Event: {changeEvent.EventType}");
            Console.WriteLine($"Blob: {changeEvent.Subject}");
            Console.WriteLine($"Time: {changeEvent.EventTime}");
        }
    }
}
```

✅ This prints:

```ini
Event: BlobCreated
Blob: /containers/invoices/blobs/invoice-101.pdf
Time: 2025-10-03T12:33:01Z

Event: BlobDeleted
Blob: /containers/invoices/blobs/invoice-101.pdf
Time: 2025-10-03T12:34:56Z
```

---

### Option 2: Python SDK

```python
from azure.storage.blob.changefeed import ChangeFeedClient

connection_string = "<your-connection-string>"
client = ChangeFeedClient.from_connection_string(connection_string)

# Iterate over change feed events
for event in client.list_changes():
    print(f"Event: {event.event_type}")
    print(f"Blob: {event.subject}")
    print(f"Time: {event.event_time}")
```

✅ Output:

```ini
Event: BlobCreated
Blob: /containers/invoices/blobs/invoice-101.pdf
Time: 2025-10-03T12:33:01Z
```

---

## 🟢 Step 5: Replay from Checkpoint

Change Feed supports **resume tokens** so you don’t re-read everything.

### .NET Example

```csharp
Pageable<BlobChangeFeedEvent> changes = changeFeedClient.GetChanges(start: DateTimeOffset.UtcNow.AddDays(-1));

foreach (BlobChangeFeedEvent changeEvent in changes)
{
    Console.WriteLine($"Event: {changeEvent.EventType}");
}
```

👉 This reads changes only from the **last 24 hours**.

---

## 🟢 Step 6: Use Cases

- 📊 **Data Lake ETL**: Replay all events into Azure Data Lake for analysis.
- 🔍 **Auditing**: Track every create/update/delete of sensitive blobs.
- ⚡ **Downstream Processing**: Build pipelines in **Azure Functions** or **Databricks** to consume change feed.
- 🛠 **Disaster Recovery**: Replay change feed into a secondary storage account.

---

## ✅ Summary

- Change Feed = **append-only log of blob changes**.
- Stored in **\$blobchangefeed** container.
- Durable, ordered, replayable.
- Access via **SDK** or direct blob reads.
- Use it for **auditing, pipelines, analytics, replay scenarios**.
- Complements **Event Grid** (push now) with **persistent log** (replay later).
