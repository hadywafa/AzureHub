# AKS Ingress Internals

Youâ€™re asking:

> **â€œIf Azure LB forwards traffic to NodeIP:80, and no process is listening on port 80 on the node â€” why does it NOT fail?â€**

This is the **single most important AKS networking insight**, and the answer is:

> **Because Linux iptables intercepts the packet _before_ any process lookup happens.**

No magic. No Azure trick. No hidden NGINX on the node.

Letâ€™s explain this **exactly in your preferred deep-internals style**.

---

## ğŸ§  **Why NodeIP:80 Works Even When Nothing Is Listening There (AKS Ingress Internals)**

> In AKS, traffic forwarded by Azure Load Balancer to `NodeIP:80` is **never delivered to a process listening on port 80**.
> It is **rewritten by kube-proxy using iptables DNAT rules** and redirected to the **NodePort (30644 / 30551)** _before_ the kernel attempts to match a listening socket.

---

<div align="center" style="background-color:#2b3436ff; border-radius:10px; border:2px solid">

```mermaid
graph TD
    A([ğŸŒ Internet Client])
    B[â˜ï¸ Azure Load Balancer]
    C[ğŸ–¥ï¸ Node NIC]
    D[ğŸ”¥ iptables PREROUTING]
    E[ğŸšª NodePort 30644]
    F[ğŸ§­ ingress-nginx Pod]
    G[/healthz or HTTP path]

    A -->|TCP 80| B
    B -->|TCP 80| C
    C --> D
    D -->|DNAT 80 â†’ 30644| E
    E --> F --> G
```

</div>

---

## ğŸ”´ **The False Assumption (Very Common)**

> â€œIf nothing listens on port 80, the request must fail.â€

This would be true **only if iptables did not exist**.

But Kubernetes **depends entirely on iptables**.

---

## ğŸŸ¢ **What Actually Happens (Kernel-Level Timeline)**

Letâ€™s walk the packet **step by step** â€” no abstractions.

---

## 1ï¸âƒ£ Azure Load Balancer forwards the packet

Azure LB rule:

```ini
Frontend : 20.x.x.x:80
Backend  : NodeIP:80
```

So Azure sends:

```ini
TCP packet â†’ NodeIP:80
```

âœ” This part is **exactly as you think**

---

## 2ï¸âƒ£ Packet enters Linux kernel (IMPORTANT)

The packet arrives at the nodeâ€™s NIC and enters the **Linux networking stack**.

### Order of operations (critical):

1. Packet hits **iptables PREROUTING**
2. DNAT rules are applied
3. THEN the kernel checks:

   - â€œIs any process listening on this port?â€

ğŸ‘‰ **Port lookup happens AFTER iptables**

---

## 3ï¸âƒ£ kube-proxy iptables rule rewrites the packet

kube-proxy has already installed rules like this:

```text
-A KUBE-SERVICES -d <NodeIP> -p tcp --dport 80 \
  -j KUBE-NODEPORTS
```

And then:

```text
-A KUBE-NODEPORTS -p tcp --dport 80 \
  -j DNAT --to-destination <NodeIP>:30644
```

So the kernel rewrites:

```ini
DESTINATION: NodeIP:80
â†’ NodeIP:30644
```

âš ï¸ This happens **before** socket lookup.

---

## 4ï¸âƒ£ NOW the kernel looks for a listener

After DNAT:

```ini
NodeIP:30644
```

And **something _is_ listening there**:

âœ” kube-proxy
âœ” ingress-nginx Service
âœ” NodePort

So the packet is accepted.

---

## 5ï¸âƒ£ Packet reaches ingress-nginx pod

Flow continues:

```ini
NodePort â†’ Service â†’ Pod â†’ NGINX
```

NGINX processes:

- `/healthz`
- `/app-a`
- `/app-b`

---

## ğŸ”¥ **This Is Why Nothing Must Listen on Port 80**

| Port      | Listener      |
| --------- | ------------- |
| 80        | âŒ No process |
| 30644     | âœ… NodePort   |
| 30551     | âœ… NodePort   |
| Pod ports | âœ… NGINX      |

Port 80 is **just a trapdoor**, not a real socket.

---

## ğŸ§  **Key Insight (Memorize This Sentence)**

> **In Kubernetes, Service ports do not require a process listener â€” iptables rewrites traffic to NodePorts before socket matching occurs.**

---

## ğŸ” **Why Health Probes Use NodePort but Traffic Uses Port 80**

Now your earlier confusion should click:

| Purpose        | Port          |
| -------------- | ------------- |
| Client traffic | 80 / 443      |
| Health probe   | 30644 / 30551 |

Because:

- Health probes must hit **something real**
- Traffic can be rewritten

---

## ğŸ§  **Ownership Table (No Confusion Left)**

| Component           | Owns                 |
| ------------------- | -------------------- |
| Azure Load Balancer | IP + frontend port   |
| kube-proxy          | iptables DNAT        |
| NodePort            | Stable backend entry |
| ingress-nginx       | HTTP routing         |
| Pod                 | Application logic    |

---

## ğŸ§  **Mental Model (This Is the Final Lock)**

> **Azure LB delivers packets to the node.
> iptables decides where they _actually_ go.
> Processes are consulted last.**

---

## ğŸ§  **Mnemonic â€” â€œIPTABLES FIRST, PROCESS LASTâ€**

If you remember nothing else, remember this.

---

## ğŸ¯ **Interview-Grade Answer**

> _Traffic forwarded by Azure Load Balancer to NodeIP:80 succeeds because kube-proxy installs iptables DNAT rules that redirect packets to the NodePort before the kernel attempts to match a listening process._

---

## ğŸ§ª **Optional Proof (If You Want to Go Even Deeper)**

On a node:

```bash
sudo iptables -t nat -L -n | grep 30644
```

You will **see the truth** printed.

---

## ğŸ§  **Who Creates the iptables Rules in AKS (and Why)?**

> The iptables rules that redirect
> `NodeIP:80 â†’ NodeIP:30644 â†’ ingress-nginx pod`
> are created and continuously reconciled by **kube-proxy**, a core Kubernetes networking component.

---

<div align="center" style="background-color:#2b3436ff; border-radius:10px; border:2px solid">

```mermaid
graph TD
    A[ğŸ§© Kubernetes Service<br/>type=LoadBalancer]
    B[ğŸ§  kube-proxy<br/>(on every node)]
    C[ğŸ”¥ iptables rules]
    D[ğŸšª NodePort 30644]
    E[ğŸ§­ ingress-nginx Pod]

    A -->|Service definition| B
    B -->|programs| C
    C -->|DNAT| D
    D --> E
```

</div>

---

## ğŸ”‘ **The One-Sentence Truth (Memorize This)**

> **kube-proxy is the component that turns Kubernetes Services into iptables rules on each node.**

Everything else (Azure LB, Ingress, Pods) depends on this.

---

## 1ï¸âƒ£ What is `kube-proxy` really?

`kube-proxy` is:

- A **daemon** running on **every node**
- Part of **Kubernetes core**
- Responsible for **Service networking**

In AKS:

- Runs as a **DaemonSet**
- One pod per node
- Has **root privileges**
- Programs **iptables** (or IPVS / eBPF)

---

## 2ï¸âƒ£ What kube-proxy watches

kube-proxy watches the Kubernetes API for:

- Services
- Endpoints / EndpointSlices

Specifically:

```text
Service type: ClusterIP
Service type: NodePort
Service type: LoadBalancer
```

Ingress objects âŒ
Azure Load Balancer âŒ
Pods directly âŒ

---

## 3ï¸âƒ£ What happens when you created this Service

```yaml
kind: Service
type: LoadBalancer
ports:
  - port: 80
```

Kubernetes internally **always** does this:

> **LoadBalancer service = NodePort + cloud integration**

So Kubernetes auto-assigns:

```ini
port: 80
nodePort: 30644
```

---

## 4ï¸âƒ£ kube-proxy reaction (this is the key moment)

On **each node**, kube-proxy does:

```text
1. Read Service definition
2. See port 80 + nodePort 30644
3. Read endpoints (ingress-nginx pods)
4. Program iptables
```

---

## 5ï¸âƒ£ The iptables rules kube-proxy installs (conceptual)

In the `nat` table:

```text
PREROUTING
 â””â”€ KUBE-SERVICES
     â””â”€ KUBE-NODEPORTS
         â””â”€ DNAT 80 â†’ 30644
```

And then:

```text
DNAT 30644 â†’ PodIP:80
```

âš ï¸ Important:

- This happens **before** the kernel checks for a listening process
- Thatâ€™s why port 80 doesnâ€™t need a listener

---

## 6ï¸âƒ£ Who does **NOT** create these iptables rules

Letâ€™s be explicit (this removes all confusion):

| Component             | Creates iptables? | Reason                 |
| --------------------- | ----------------- | ---------------------- |
| Azure Load Balancer   | âŒ                | Cloud, external        |
| ingress-nginx         | âŒ                | Runs inside pods       |
| Kubernetes API server | âŒ                | Control plane only     |
| Container runtime     | âŒ                | Containers only        |
| Linux OS              | âŒ                | Passive                |
| **kube-proxy**        | âœ…                | **Service networking** |

---

## 7ï¸âƒ£ Why kube-proxy MUST run on every node

Because:

- Traffic can land on **any node**
- iptables rules must exist **locally**
- Azure LB distributes traffic blindly

If kube-proxy is missing on a node:

- That node is **dead for Services**
- Even if pods are running

---

## 8ï¸âƒ£ Relationship between kube-proxy and Azure LB (clear boundary)

| Responsibility          | Owner         |
| ----------------------- | ------------- |
| Public IP               | Azure LB      |
| Node selection          | Azure LB      |
| Packet delivery to node | Azure LB      |
| Port rewriting          | kube-proxy    |
| Pod selection           | kube-proxy    |
| HTTP routing            | ingress-nginx |

They are **complementary**, not overlapping.

---

## ğŸ§  **Mental Model (Final Lock)**

> **Azure LB gets traffic to the node.
> kube-proxy gets traffic to the pod.**

Ingress just decides _which_ pod.

---

## ğŸ§  **Mnemonic â€” â€œLB â†’ NODE, PROXY â†’ PODâ€**

- Load Balancer â†’ node
- kube-proxy â†’ pod

---

## ğŸ¯ **Interview-Grade Answer**

> _The iptables rules that redirect traffic from Service ports to NodePorts and pod endpoints are created and maintained by kube-proxy, which runs on every Kubernetes node and implements Service networking._

---
